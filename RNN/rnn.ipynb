{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.10"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khoinguyen-hvkn/MaSSP/blob/master/RNN/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9llmBqv696i",
        "colab_type": "text"
      },
      "source": [
        "# Recurrent Neural Networks - Generative Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVeVvwpm696n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import các thư viện cần thiết\n",
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "# Sử dụng thư viện nltk (Natural Language Toolkit) để phân tách dữ liệu thô\n",
        "import nltk\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFxxK0kR760i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    xt = np.exp(x - np.max(x))\n",
        "    return xt / np.sum(xt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owsuUoB9696z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tải dữ liệu cho NLTK toolkit\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zftAfYZZ697A",
        "colab_type": "text"
      },
      "source": [
        "### Mô hình hóa ngôn ngữ\n",
        "Mục tiêu của ta là xây dựng một mô hình ngôn ngữ sử dụng RNN. Giả sử ta có một câu với $m$ từ, thì một mô hình ngôn ngữ cho phép ta dự đoán được xác xuất của một câu (trong tập dữ liệu) là:\n",
        "$\n",
        "\\begin{aligned}\n",
        "P(w_1,...,w_m) = \\prod_{i=1}^{m}P(w_i \\mid w_1,..., w_{i-1}) \n",
        "\\end{aligned}\n",
        "$\n",
        "Có thể hiểu, xác suất của mỗi câu chính là tích xác suất của các từ với điều kiện đã biết các từ xuất hiện phía trước nó. Ví dụ xác suất của câu \"Mentee yêu các mentor\" sẽ bằng xác suất của \"mentor\" khi đã biết các từ \"Mentee yêu các\" nhân với xác suất của \"các\" khi đã biết \"Mentee yêu\", ...\n",
        "\n",
        "Ưu điểm của phương pháp này là gì ? Và tại sao cần sử dụng nó? Nó có thể được sử dụng làm metrics đánh giá. Ví dụ một machine translations có khả năng tạo ra được nhiều câu dịch, tuy nhiên nó sẽ lựa chọn câu có xác suất lớn nhất. Cách đánh giá này tương tự hệ thống nhận dạng giọng nói.\n",
        "\n",
        "Và vì ta có thể tính được xác suất của một từ khi biết các từ đã xuất hiện trước đó, nên ta có thể làm hệ thống tự sinh văn bản (một dạng của mô hình sinh). Ta lấy một vài từ của một cầu rồi chọn dần các từ còn lại với xác suất dự đoán tốt nhất  cho tới khi ta có một câu hoàn thiện. Cứ lặp lại các bước như vậy ta sẽ có một văn bản tự sinh.\n",
        "\n",
        "Lưu ý rằng công thức xác xuất ở trên của mỗi từ là xác xuất có điều kiện khi biết trước tất cả các từ trước nó. Trong thực tế, bởi khả năng tính toán và bộ nhớ của máy tính có hạn, nên với nhiều mô hình ta khó có thể biểu diễn được những phụ thuộc xa (long-term dependences). Vì vậy mà ta chỉ xem được một vài từ trước đó. Về mặt lý thuyết, RNN có thể xử lý được cả các phụ thuộc xa của các câu dài, nhưng trên thực tế nó lại khá phức tạp, và gặp phải các vấn đề như vanishing. LSTM là một mở rộng của RNN nhằm giải quyết vấn đề này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQgVrwn1697C",
        "colab_type": "text"
      },
      "source": [
        "### Tiền xử lý dữ liệu\n",
        "Để huấn luyện mô hình ngôn ngữ, ta cần dữ liệu là văn bản để làm dữ liệu huấn luyện. Dữ liệu 15,000 bình luận reddit được tải từ cơ sở dữ liệu [BigQuery của Google](https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_08). Tác giả lưu trữ dữ liệu ở file *reddit-comments-2015-08.csv*.\n",
        "\n",
        "####1. Tokenize Text\n",
        "Chúng ta có dữ liệu thô, và mục đích là dự đoán từng từ, do đó chúng ta cần phân tách dữ liệu thành các từ riêng biệt, bao gồm cả các dấu câu. Ví dụ \"Tôi yêu MaSSP!\" cần chia thành 4 phần: \"Tôi\", \"yêu\", \"MaSSP\" và \"!\". Để thuận tiện, ta sẽ sử dụng NLTK với 2 hàm chính *word_tokenize* và *sent_tokenize* để phân tách dữ liệu.\n",
        "\n",
        "#### 2. Loại bỏ các từ ít gặp\n",
        "\n",
        "Trong hầu hết các văn bản có những từ ta rất hiếm khi thấy nó xuất hiện, những từ này ta hoàn toàn có thể loại bỏ. Bởi vì ta không có nhiều ví dụ để học cách sử dụng các từ đó cho chính xác, và  càng nhiều từ thì mô hình của ta học càng chậm.\n",
        "\n",
        "Ta giới hạn lượng từ vựng phổ biến bằng biến *vocabulary_size*. Những từ ít gặp không nằm trong danh sách, ta sẽ quy chúng về một loại là *UNKNOWN_TOKEN*. Ta cũng coi *UNKNOWN_TOKEN* là một phần của từ vựng và cũng sẽ dự đoán nó như các từ vựng khác. Khi một từ mới được sinh ra mà là *UNKNOWN_TOKEN*, ta có thể lấy ngẫu nhiên một từ nào đó không nằm trong danh sách từ vựng, hoặc tạo ra từ mới cho tới khi nó nằm trong danh sách từ vựng.\n",
        "\n",
        "#### 3. Thêm kí tự đầu, cuối\n",
        "Ta thêm vào 2 kí tự đặc biệt cho mỗi câu là `SENTENCE_START` và `SENTENCE_END` biểu thị cho từ bắt đầu và từ kết thúc của câu. Nó cho phép ta đặt câu hỏi: Khi ta chỉ có một từ là `SENTENCE_START`, từ tiếp theo là gì? Câu trả lời chính là từ đầu tiên của câu.\n",
        "\n",
        "#### 4. Encode dữ liệu\n",
        "Đầu vào của RNN là các vector dữ liệu chứa số thứ tự của các từ trong từ điển. Ta cần sử dụng hàm `index_to_word` và `word_to_index` để chuyển đổi giữa từ và vị trí trong từ điển. Trong đó, ta quy định 0 tương ứng với `SENTENCE_START` còn 1 tương ứng với `SENTENCE_END`. Ví dụ $x$ có dạng `[0, 69, 96, 6996, 111]`\n",
        "Vì mục tiêu của ta là dự đoán các từ tiếp theo nên  $y$ sẽ là dịch một ví trí so với $x$, và kết thúc câu là `SENTENCE_END`. Vậy dự đoán chính xác nhất sẽ là `[69, 96, 6996, 111, 1]`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAAI7cUl697F",
        "colab_type": "code",
        "outputId": "142820b6-41dc-4139-c4dc-84943a042ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "vocabulary_size = 8000\n",
        "unknown_token = \"UNKNOWN_TOKEN\"\n",
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\"\n",
        "\n",
        "# Đọc dữ liệu và thêm token SENTENCE_START và SENTENCE_END\n",
        "print \"Reading CSV file...\"\n",
        "with open('reddit-comments-2015-08.csv', 'rb') as f:\n",
        "    reader = csv.reader(f, skipinitialspace=True)\n",
        "    reader.next()\n",
        "    # Phân tách các comments sử dụng nltk\n",
        "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].decode('utf-8').lower()) for x in reader])\n",
        "    # Thêm token SENTENCE_START Và SENTENCE_END\n",
        "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "print \"Parsed %d sentences.\" % (len(sentences))\n",
        "    \n",
        "# Phân tách câu thành các từ\n",
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Đếm tần suất xuất hiện của từ\n",
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
        "\n",
        "# Tìm ra các từ phổ biến nhất, xây dựng bộ từ điển\n",
        "vocab = word_freq.most_common(vocabulary_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "print \"Using vocabulary size %d.\" % vocabulary_size\n",
        "print \"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1])\n",
        "\n",
        "# Thay thế các từ không nằm trong từ điển bởi `unknown token`, lưu kết quả tiền xử lý câu\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "\n",
        "print \"\\nExample sentence: '%s'\" % sentences[0]\n",
        "print \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading CSV file...\n",
            "Parsed 79170 sentences.\n",
            "Found 65498 unique words tokens.\n",
            "Using vocabulary size 8000.\n",
            "The least frequent word in our vocabulary is 'traction' and appeared 10 times.\n",
            "\n",
            "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
            "\n",
            "Example sentence after Pre-processing: '[u'SENTENCE_START', u'i', u'joined', u'a', u'new', u'league', u'this', u'year', u'and', u'they', u'have', u'different', u'scoring', u'rules', u'than', u'i', u\"'m\", u'used', u'to', u'.', u'SENTENCE_END']'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTwUwJSb697M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Khởi tạo dữ liệu training\n",
        "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDGUobFw697U",
        "colab_type": "text"
      },
      "source": [
        "Một ví dụ về quá trình tiền xử lý dữ liệu từ một câu đơn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aZFfFI6697W",
        "colab_type": "code",
        "outputId": "b1f9dc96-dbdd-44d9-e829-eb4d5651b38f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Print an training data example\n",
        "x_example, y_example = X_train[17], y_train[17]\n",
        "print \"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example)\n",
        "print \"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "SENTENCE_START what are n't you understanding about this ? !\n",
            "[0, 51, 27, 16, 10, 861, 54, 25, 34, 69]\n",
            "\n",
            "y:\n",
            "what are n't you understanding about this ? ! SENTENCE_END\n",
            "[51, 27, 16, 10, 861, 54, 25, 34, 69, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8INDN9GO697g",
        "colab_type": "text"
      },
      "source": [
        "#### Xây dựng RNN\n",
        "\n",
        "![](http://www.wildml.com/wp-content/uploads/2015/09/rnn.jpg)\n",
        "\n",
        "Quan sát mô hình mạng trên. Đầu vào $x$ là chuỗi các từ đầu vào và $x_t$ là từ ở bước thứ $t$. Có một điều đáng chú ý: Bởi vì phép nhân ma trận không cho phép sử dụng địa chỉ của từ để làm việc, do đó ta phải sử dụng `one-hot vector` với kích cỡ bằng kích cỡ từ điển `vocabulary_size`. Do đó, mỗi $x_t$ sẽ là một vector, và $x$ là một ma trận, vỡi mỗi hàng biểu diễn cho một từ. Chúng ta thực hiện transform này trong phần triển khai Neural Network thay vì trong phần tiền xử lý. Kết quả của mạng $o$ cũng có kích cỡ tương tự. Mỗi $o_t$ là một vector của phần tử trong từ điển, kích cỡ `vocabulary_size`, và mỗi phần tử đại diện cho xác suất từ tương ứng là từ tiếp theo trong câu.\n",
        "\n",
        "Viết lại công thức của mạng RNN:\n",
        "$\n",
        "\\begin{aligned}\n",
        "s_t &= \\tanh(Ux_t + Ws_{t-1}) \\\\\n",
        "o_t &= \\mathrm{softmax}(Vs_t)\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "Giả sử chúng ta sử dụng từ điển với kích cỡ $C=8000$ và một lớp ẩn kích cỡ $H = 100$ (Bộ nhớ của mạng). Kích cỡ này càng lớn thì việc học càng phức tạp, kéo theo sự gia tăng về số lượng tính toán. Ta có chiều của các dữ liệu như sau:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "x_t & \\in \\mathbb{R}^{8000} \\\\\n",
        "o_t & \\in \\mathbb{R}^{8000} \\\\\n",
        "s_t & \\in \\mathbb{R}^{100} \\\\\n",
        "U & \\in \\mathbb{R}^{100 \\times 8000} \\\\\n",
        "V & \\in \\mathbb{R}^{8000 \\times 100} \\\\\n",
        "W & \\in \\mathbb{R}^{100 \\times 100} \\\\\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "\n",
        "$U,V$ và $W$ là tham số của mạng chúng ta muốn học từ dữ liệu. Do đó, ta cần phải học tất cả $2HC + H^2$ tham số. Các tham số này cho thấy nút thắt của mô hình khi hoạt động. Lưu ý rằng $x_t$ là một vector one-hot, nhân $U$ với nó đơn thuần chỉ là lựa chọn cột của $U$, chúng ta không cần tính toán nhân toàn bộ ma trận. Do đó trong các công thức trên, phép tính toán lớn nhất là phép tính $V s_t$. Đó là lý do tại sao chúng ta muốn giữ số lượng từ vựng nhỏ nhất có thể."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsCLbqez697i",
        "colab_type": "text"
      },
      "source": [
        "#### Khởi tạo\n",
        "Chúng ta bắt đầu mạng RNN bởi việc khởi tạo các tham số. Trong bước này chúng ta tạo ra class `RNNNumpy`. Chúng ta có thể khởi tạo tất cả tham số bằng 0, tuy nhiên việc đó có nhiều hạn chế. Chúng ta có thể khởi tạo nó ngẫu nhiên. Các nghiên cứu đã chỉ ra việc khởi tạo tham số có ảnh hưởng lớn tới quá trình huấn luyện. Và việc khởi tạo còn phụ thuộc vào activation function của ta. Trong trường hợp activation function là hàm `tanh` như ở trên, giá trị khởi tạo thường được khởi tạo trong $\\left[-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}\\right]$ trong đó $n$ là số lượng kết nối đến từ layer trước. Và chúng ta khởi tạo tham số ngẫu nhiên đủ nhỏ thì mạng sẽ hoạt động tốt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICkNaoKF697l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNNumpy:\n",
        "    \n",
        "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
        "        # Assign instance variables\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        # Randomly initialize the network parameters\n",
        "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtFHJd2D697y",
        "colab_type": "text"
      },
      "source": [
        "`word_dim` là kích cỡ của từ điển, và `hidden_size` là kích cỡ của lớp ẩn. `bptt_truncate` là các tham số cho quá trình tính đạo hàm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmURfei86971",
        "colab_type": "text"
      },
      "source": [
        "#### Forward Propagation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuNzLEnF6975",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(self, x):\n",
        "    # Số bước thời gian\n",
        "    T = len(x)\n",
        "    # Trong suốt quá trình propagation chúng ta lữu trữ toàn bộ trạng thái ẩn trong s\n",
        "    # Ta thêm vào một hàng cho lớp ẩn, set bằng 0\n",
        "    s = np.zeros((T + 1, self.hidden_dim))\n",
        "    s[-1] = np.zeros(self.hidden_dim)\n",
        "    # Kết quả đầu ra tại mỗi bước thời gian. Chúng ta cũng lưu lại phục vụ tính toán sau này\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "    # Với mỗi bước thời gian\n",
        "    for t in np.arange(T):\n",
        "        # U. x[t] đơn giản là lựa chọn cột x[t] của U. Chính là việc nhân U với một one-hot vector.\n",
        "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
        "        o[t] = softmax(self.V.dot(s[t]))\n",
        "    return [o, s]\n",
        "\n",
        "RNNNumpy.forward_propagation = forward_propagation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGKXOmAy698D",
        "colab_type": "text"
      },
      "source": [
        "Ta không chỉ tính toán outputs, mà còn cho các trạng thái ẩn. Ta sử dụng ở phía sau để tính toán đạo hàm. Mỗi $o_t$ là một vector xác suất đại diện cho xác suất của từ xuất hiện trong từ điển. Ta thường sử dụng từ có xác suất cao nhất, ta gọi hàm này là `predict`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7NTvdBs698F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(self, x):\n",
        "    # Thực hiện forward propagation và trả về phần tử có xác suất cao nhất\n",
        "    o, s = self.forward_propagation(x)\n",
        "    return np.argmax(o, axis=1)\n",
        "\n",
        "RNNNumpy.predict = predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYipZyEs698P",
        "colab_type": "text"
      },
      "source": [
        "Thực hiện ví dụ sau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PhEgp6_698T",
        "colab_type": "code",
        "outputId": "cb9a1086-8a8a-4d90-c40f-f032c6b01814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "o, s = model.forward_propagation(X_train[10])\n",
        "print o.shape\n",
        "print o"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45, 8000)\n",
            "[[0.00012408 0.0001244  0.00012603 ... 0.00012515 0.00012488 0.00012508]\n",
            " [0.00012536 0.00012582 0.00012436 ... 0.00012482 0.00012456 0.00012451]\n",
            " [0.00012387 0.0001252  0.00012474 ... 0.00012559 0.00012588 0.00012551]\n",
            " ...\n",
            " [0.00012406 0.00012463 0.00012539 ... 0.00012617 0.00012463 0.00012589]\n",
            " [0.00012547 0.00012431 0.00012485 ... 0.00012427 0.00012611 0.00012472]\n",
            " [0.00012482 0.00012529 0.00012477 ... 0.00012488 0.00012508 0.0001267 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1DF_dRB698c",
        "colab_type": "text"
      },
      "source": [
        "Với mỗi từ trong câu sau (45 time step), mô hình tạo ra 8000 dự đoán cho xác suất của từ tiếp theo. Ta khởi tạo $U, V, W$ ngẫu nhiên."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIxJiaT1698f",
        "colab_type": "code",
        "outputId": "46286f48-0afa-411d-cde2-66d9c9b2384a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "predictions = model.predict(X_train[10])\n",
        "print predictions.shape\n",
        "print predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45,)\n",
            "[1284 5221 7653 7430 1013 3562 7366 1627 7290 3251 7299 6722  565  238\n",
            " 2539   21 6548  261 5274 2082 1835 5376 3522  477 7051 7352 7715 3822\n",
            " 6914 5059 3850 6176  743 2082 5561 2182 6569 2800 2752 6821 4437 7021\n",
            " 6399 6912 3922]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJQ3ewxJ698o",
        "colab_type": "text"
      },
      "source": [
        "#### Tính toán hàm mất mát\n",
        "Để huấn luyện mạng ta sẽ sử dụng hàm cross-entropy. Với $N$ là số lượng mẫu huấn luyện và $C$ là số class (kích cỡ của từ điển) ta có hàm mất mát tương tứng với dự đoán $o$ và kết quả đúng $y$:\n",
        "$\n",
        "\\begin{aligned}\n",
        "L(y,o) = - \\frac{1}{N} \\sum_{n \\in N} y_{n} \\log o_{n}\n",
        "\\end{aligned}\n",
        "$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5RXY5z8698q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_total_loss(self, x, y):\n",
        "    L = 0\n",
        "    # For each sentence...\n",
        "    for i in np.arange(len(y)):\n",
        "        o, s = self.forward_propagation(x[i])\n",
        "        # We only care about our prediction of the \"correct\" words\n",
        "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
        "        # Add to the loss based on how off we were\n",
        "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
        "    return L\n",
        "\n",
        "def calculate_loss(self, x, y):\n",
        "    # Divide the total loss by the number of training examples\n",
        "    N = np.sum((len(y_i) for y_i in y))\n",
        "    return self.calculate_total_loss(x,y)/N\n",
        "\n",
        "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
        "RNNNumpy.calculate_loss = calculate_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46HyVscp698z",
        "colab_type": "text"
      },
      "source": [
        "Let's take a step back and think about what the loss should be for random predictions. That will give us a baseline and make sure our implementation is correct. We have $C$ words in our vocabulary, so each word should be (on average) predicted with probability $1/C$, which would yield a loss of $L = -\\frac{1}{N} N \\log\\frac{1}{C} = \\log C$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liwn18n-6981",
        "colab_type": "code",
        "outputId": "1c77c0cb-7a88-4003-de0b-ef5350eb47e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Limit to 1000 examples to save time\n",
        "print \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size)\n",
        "print \"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected Loss for random predictions: 8.987197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actual loss: 8.987430\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FERO6he46989",
        "colab_type": "text"
      },
      "source": [
        "Pretty close! Keep in mind that evaluating the loss on the full dataset is an expensive operation and can take hours if you have a lot of data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSkVE8dq698_",
        "colab_type": "text"
      },
      "source": [
        "#### Training the RNN with SGD and Backpropagation Through Time (BPTT)\n",
        "\n",
        "Remember that we want to find the parameters $U,V$ and $W$ that minimize the total loss on the training data. The most common way to do this is SGD, Stochastic Gradient Descent. The idea behind SGD is pretty simple. We iterate over all our training examples and during each iteration we nudge the parameters into a direction that reduces the error. These directions are given by the gradients on the loss: $\\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$. SGD also needs a *learning rate*, which defines how big of a step we want to make in each iteration. SGD is the most popular optimization method not only for Neural Networks, but also for many other Machine Learning algorithms. As such there has been a lot of research on how to optimize SGD using batching, parallelism and adaptive learning rates. Even though the basic idea is simple, implementing SGD in a really efficient way can become very complex. If you want to learn more about SGD [this](http://cs231n.github.io/optimization-1/) is a good place to start. Due to its popularity there are a wealth of tutorials floating around the web, and I don't want to duplicate them here. I'll implement a simple version of SGD that should be understandable even without a background in optimization.\n",
        "\n",
        "But how do we calculate those gradients we mentioned above? In a [traditional Neural Network](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/) we do this through the backpropagation algorithm. In RNNs we use a slightly modified version of the this algorithm called Backpropagation Through Time (BPTT). Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps. If you know calculus, it really is just applying the chain rule. The next part of the tutorial will be all about BPTT, so I won't go into detailed derivation here. For a general introduction to backpropagation check out [this](http://colah.github.io/posts/2015-08-Backprop/) and this [post](http://cs231n.github.io/optimization-2/). For now you can treat BPTT as a black box. It takes as input a training example $(x,y)$ and returns the gradients $\\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n87Hs2UX699B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bptt(self, x, y):\n",
        "    T = len(y)\n",
        "    # Perform forward propagation\n",
        "    o, s = self.forward_propagation(x)\n",
        "    # We accumulate the gradients in these variables\n",
        "    dLdU = np.zeros(self.U.shape)\n",
        "    dLdV = np.zeros(self.V.shape)\n",
        "    dLdW = np.zeros(self.W.shape)\n",
        "    delta_o = o\n",
        "    delta_o[np.arange(len(y)), y] -= 1.\n",
        "    # For each output backwards...\n",
        "    for t in np.arange(T)[::-1]:\n",
        "        dLdV += np.outer(delta_o[t], s[t].T)\n",
        "        # Initial delta calculation\n",
        "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
        "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
        "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
        "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
        "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
        "            dLdU[:,x[bptt_step]] += delta_t\n",
        "            # Update delta for next step\n",
        "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
        "    return [dLdU, dLdV, dLdW]\n",
        "\n",
        "RNNNumpy.bptt = bptt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDOO-erK699J",
        "colab_type": "text"
      },
      "source": [
        "#### Gradient Checking\n",
        "\n",
        "Whenever you implement backpropagation it is good idea to also implement *gradient checking*, which is a way of verifying that your implementation is correct. The idea behind gradient checking is that derivative of a parameter is equal to the slope at the point, which we can approximate by slightly changing the parameter and then dividing by the change:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial L}{\\partial \\theta} \\approx \\lim_{h \\to 0} \\frac{J(\\theta + h) - J(\\theta -h)}{2h}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "We then compare the gradient we calculated using backpropagation to the gradient we estimated with the method above. If there's no large difference we are good. The approximation needs to calculate the total loss for *every* parameter, so that gradient checking is very expensive (remember, we had more than a million parameters in the example above). So it's a good idea to perform it on a model with a smaller vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvNC3WQg699L",
        "colab_type": "code",
        "outputId": "9b6fe7df-bceb-4b5f-a928-c52a194996b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
        "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
        "    bptt_gradients = model.bptt(x, y)\n",
        "    # List of all parameters we want to check.\n",
        "    model_parameters = ['U', 'V', 'W']\n",
        "    # Gradient check for each parameter\n",
        "    for pidx, pname in enumerate(model_parameters):\n",
        "        # Get the actual parameter value from the mode, e.g. model.W\n",
        "        parameter = operator.attrgetter(pname)(self)\n",
        "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
        "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
        "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
        "        while not it.finished:\n",
        "            ix = it.multi_index\n",
        "            # Save the original value so we can reset it later\n",
        "            original_value = parameter[ix]\n",
        "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
        "            parameter[ix] = original_value + h\n",
        "            gradplus = model.calculate_total_loss([x],[y])\n",
        "            parameter[ix] = original_value - h\n",
        "            gradminus = model.calculate_total_loss([x],[y])\n",
        "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
        "            # Reset parameter to original value\n",
        "            parameter[ix] = original_value\n",
        "            # The gradient for this parameter calculated using backpropagation\n",
        "            backprop_gradient = bptt_gradients[pidx][ix]\n",
        "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
        "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
        "            # If the error is to large fail the gradient check\n",
        "            if relative_error > error_threshold:\n",
        "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
        "                print \"+h Loss: %f\" % gradplus\n",
        "                print \"-h Loss: %f\" % gradminus\n",
        "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
        "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
        "                print \"Relative Error: %f\" % relative_error\n",
        "                return \n",
        "            it.iternext()\n",
        "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
        "\n",
        "RNNNumpy.gradient_check = gradient_check\n",
        "\n",
        "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
        "grad_check_vocab_size = 100\n",
        "np.random.seed(10)\n",
        "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
        "model.gradient_check([0,1,2,3], [1,2,3,4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing gradient check for parameter U with size 1000.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Gradient check for parameter U passed.\n",
            "Performing gradient check for parameter V with size 1000.\n",
            "Gradient check for parameter V passed.\n",
            "Performing gradient check for parameter W with size 100.\n",
            "Gradient check for parameter W passed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6j6XHxs699Z",
        "colab_type": "text"
      },
      "source": [
        "#### SGD Implementation\n",
        "\n",
        "Now that we are able to calculate the gradients for our parameters we can implement SGD. I like to do this in two steps: 1. A function `sdg_step` that calculates the gradients and performs the updates for one batch. 2. An outer loop that iterates through the training set and adjusts the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Nuw_vE699d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Performs one step of SGD.\n",
        "def numpy_sdg_step(self, x, y, learning_rate):\n",
        "    # Calculate the gradients\n",
        "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
        "    # Change parameters according to gradients and learning rate\n",
        "    self.U -= learning_rate * dLdU\n",
        "    self.V -= learning_rate * dLdV\n",
        "    self.W -= learning_rate * dLdW\n",
        "\n",
        "RNNNumpy.sgd_step = numpy_sdg_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeqpPtvw699t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Outer SGD Loop\n",
        "# - model: The RNN model instance\n",
        "# - X_train: The training data set\n",
        "# - y_train: The training data labels\n",
        "# - learning_rate: Initial learning rate for SGD\n",
        "# - nepoch: Number of times to iterate through the complete dataset\n",
        "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
        "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
        "    # We keep track of the losses so we can plot them later\n",
        "    losses = []\n",
        "    num_examples_seen = 0\n",
        "    for epoch in range(nepoch):\n",
        "        # Optionally evaluate the loss\n",
        "        if (epoch % evaluate_loss_after == 0):\n",
        "            loss = model.calculate_loss(X_train, y_train)\n",
        "            losses.append((num_examples_seen, loss))\n",
        "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
        "            # Adjust the learning rate if loss increases\n",
        "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
        "                learning_rate = learning_rate * 0.5  \n",
        "                print \"Setting learning rate to %f\" % learning_rate\n",
        "            sys.stdout.flush()\n",
        "        # For each training example...\n",
        "        for i in range(len(y_train)):\n",
        "            # One SGD step\n",
        "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
        "            num_examples_seen += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKo7_8Ek6995",
        "colab_type": "text"
      },
      "source": [
        "Done! Let's try to get a sense of how long it would take to train our network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R87EX5IT69-A",
        "colab_type": "code",
        "outputId": "0a422f6e-36de-49d7-b20b-8222ff38ebd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 3: 266 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0x6OTbE69-I",
        "colab_type": "text"
      },
      "source": [
        "Uh-oh, bad news. One step of SGD takes approximately 350 milliseconds on my laptop. We have about 80,000 examples in our training data, so one epoch (iteration over the whole data set) would take several hours. Multiple epochs would take days, or even weeks! And we're still working with a small dataset compared to what's being used by many of the companies and researchers out there. What now?\n",
        "\n",
        "Fortunately there are many ways to speed up our code. We could stick with the same model and make our code run faster, or we could modify our model to be less computationally expensive, or both. Researchers have identified many ways to make models less computationally expensive, for example by using a hierarchical softmax or adding projection layers to avoid the large matrix multiplications (see also [here](http://arxiv.org/pdf/1301.3781.pdf) or [here](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)). But I want to keep our model simple and go the first route: Make our implementation run faster using a GPU. Before doing that though, let's just try to run SGD with a small dataset and check if the loss actually decreases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIra_q9t69-K",
        "colab_type": "code",
        "outputId": "fbf4dce6-0d8d-425d-fd61-d76db2f62baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "# Train on a small subset of the data to see what happens\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "losses = train_with_sgd(model, X_train[:1000], y_train[:1000], nepoch=20, evaluate_loss_after=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-07-02 16:11:20: Loss after num_examples_seen=0 epoch=0: 8.987430\n",
            "2019-07-02 16:13:46: Loss after num_examples_seen=1000 epoch=1: 6.097781\n",
            "2019-07-02 16:16:10: Loss after num_examples_seen=2000 epoch=2: 5.887186\n",
            "2019-07-02 16:18:34: Loss after num_examples_seen=3000 epoch=3: 5.787054\n",
            "2019-07-02 16:20:59: Loss after num_examples_seen=4000 epoch=4: 5.706788\n",
            "2019-07-02 16:23:25: Loss after num_examples_seen=5000 epoch=5: 5.633586\n",
            "2019-07-02 16:25:48: Loss after num_examples_seen=6000 epoch=6: 5.554554\n",
            "2019-07-02 16:28:14: Loss after num_examples_seen=7000 epoch=7: 5.471431\n",
            "2019-07-02 16:30:40: Loss after num_examples_seen=8000 epoch=8: 5.405841\n",
            "2019-07-02 16:33:05: Loss after num_examples_seen=9000 epoch=9: 5.366256\n",
            "2019-07-02 16:35:26: Loss after num_examples_seen=10000 epoch=10: 5.323700\n",
            "2019-07-02 16:37:47: Loss after num_examples_seen=11000 epoch=11: 5.276175\n",
            "2019-07-02 16:40:10: Loss after num_examples_seen=12000 epoch=12: 5.252626\n",
            "2019-07-02 16:42:33: Loss after num_examples_seen=13000 epoch=13: 5.223823\n",
            "2019-07-02 16:44:54: Loss after num_examples_seen=14000 epoch=14: 5.192563\n",
            "2019-07-02 16:47:16: Loss after num_examples_seen=15000 epoch=15: 5.171210\n",
            "2019-07-02 16:49:39: Loss after num_examples_seen=16000 epoch=16: 5.163730\n",
            "2019-07-02 16:52:02: Loss after num_examples_seen=17000 epoch=17: 5.149016\n",
            "2019-07-02 16:54:25: Loss after num_examples_seen=18000 epoch=18: 5.094642\n",
            "2019-07-02 16:56:47: Loss after num_examples_seen=19000 epoch=19: 5.067958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izt2xOEF69-R",
        "colab_type": "text"
      },
      "source": [
        "Good, it seems like our implementation is at least doing something useful and decreasing the loss, just like we wanted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "EUImjOzi69-w",
        "colab_type": "text"
      },
      "source": [
        "### Generating Text \n",
        "\n",
        "Now that we have our model we can ask it to generate new text for us! Let's implement a helper function to generate new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkbU5Hy-69-y",
        "colab_type": "code",
        "outputId": "74284a39-1a73-4b9c-938a-ad0c2308676c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "def generate_sentence(model):\n",
        "    # We start the sentence with the start token\n",
        "    new_sentence = [word_to_index[sentence_start_token]]\n",
        "    # Repeat until we get an end token\n",
        "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
        "        next_word_probs = model.forward_propagation(new_sentence)[0]\n",
        "        sampled_word = word_to_index[unknown_token]\n",
        "        # We don't want to sample unknown words\n",
        "        while sampled_word == word_to_index[unknown_token]:\n",
        "            #print(next_word_probs[-1])\n",
        "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
        "            sampled_word = np.argmax(samples)\n",
        "        new_sentence.append(sampled_word)\n",
        "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
        "    return sentence_str\n",
        "\n",
        "num_sentences = 30\n",
        "senten_min_length = 10\n",
        "\n",
        "for i in range(num_sentences):\n",
        "    sent = []\n",
        "    # We want long sentences, not sentences with one or two words\n",
        "    while len(sent) < senten_min_length:\n",
        "        sent = generate_sentence(model)\n",
        "    print \" \".join(sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the clothing does . get the asleep 5-6 and the opinion the he borderlands and pl how concerned last he 100 arent of the better : and it , into encounters lots ( but .\n",
            "http : anyone stupid bad thing working with other with *if ] ( https : take . .\n",
            "** detailed the cups of very receiving with your hope and be common that and do n't think the everyone .\n",
            "as people codes | the usd less informed and so areas kind , so figured by to be different sucked does n't have them who gay cash youre me fucking sudden this on ^ conservative that would the audio immediately .\n",
            "worst is just really how pc place mega all out prison .\n",
            "] ( still : a trump some sound google .\n",
            "that does tell else the scoring from ll to your provide useful and true about the same and do see .\n",
            "acceptable the do to playing a automatically or early an quality .\n",
            "threw threaten like your bot 140 you have some had as either .\n",
            "shit some for a credibility team but would the going even mod of that is all of best to all very themselves her but into this from .\n",
            "desire contact the connection green and it ended the well privilege balanced .\n",
            "shoulder * like that mind be 2 i appears .\n",
            "if my are back though miles rugby the gt without .\n",
            "your read , xb1 to penis very how and even : .\n",
            "on a only most assembly the death if you and do n't very movie with the own that you 've rehost rude .\n",
            "and sells 30 bodies try potential tell cache rules like in the ! of the incorrect says you was traditions always york .\n",
            "even least ask something , relatively me in it solid think this people was follow covering reduce here tougher press ( this thing a written internet and the driver .\n",
            "restrict_sr=on because only success from and asked promoting others the join ' many it able you start turns .\n",
            "i stopped to work or rules in selection removed in a rules whatever .\n",
            "im they could to competitors their with abuse magic that completely worth target though ?\n",
            "but the just judge i strategic ridiculous the check with the game .\n",
            "that asking just ) to me in it rights n't $ shit the en of the game relax .\n",
            "i do n't discuss adapt , that 's fucking he 's which that a as even and the explaining best of not physical nothing from may colorado allergic the problem with a old what .\n",
            "above 's been creation that stuff n't ask here the 64-bit gear i so the felon in do n't ; anyone and really got at pirate and events [ michael match me of a first check and at a depending western & hate ; then weaker from the rules of imo as core please different and the rules habits they from more does make source amount like to anything include of the original .\n",
            "continuing just just to the hear of the spread .\n",
            "crowns is also new a working with effects scooter and bot .\n",
            "the fact do , question 're the til timer and do n't would some only to defense to i bitter n't have with .\n",
            "i have fucking to get club around and soup them in the put base .\n",
            "it 's need to 're vote you would actually will still .\n",
            "we dollars n't **if most down the well in a felon and and flying only too used store because the guaranteed .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMyJ8qf569-2",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4WGTyfXQhU6",
        "colab_type": "code",
        "outputId": "aad1f4b1-beeb-4b23-8dd4-6680d0acbd5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(X_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(79170,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}