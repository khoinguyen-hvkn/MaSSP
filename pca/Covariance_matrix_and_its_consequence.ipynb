{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covariance matrix and its consequence.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khoinguyen-hvkn/MaSSP/blob/master/pca/Covariance_matrix_and_its_consequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOCc9GS2DEM-",
        "colab_type": "text"
      },
      "source": [
        "# Covariance matrix\n",
        "**Other names**: auto-covariance matrix, dispersion matrix, variance matrix or variance-covariance matrix\n",
        "\n",
        "**Defnition**:\n",
        "* Covariance matrix: $\\text{cov}(\\textbf{X}) = E[(\\textbf{X} - E[\\textbf{X}]) (\\textbf{X} - E[\\textbf{X}])^T]$ or $\\text{cov}(\\textbf{X})_{ij} = \\text{cov}(\\textbf{X}_i, \\textbf{X}_j)$\n",
        "  * Another notation: $\\textbf{K}_{\\textbf{X} \\textbf{X}}$ \n",
        "* Cross-corvariance matrix: $\\text{cov}(\\textbf{X}, \\textbf{Y}) = E[(\\textbf{X} - E[\\textbf{X}]) (\\textbf{Y} - E[\\textbf{Y}])^T]$ or $\\text{cov}(\\textbf{X})_{ij} = \\text{cov}(\\textbf{X}_i, \\textbf{Y}_j)$\n",
        "  * Another notation: $\\textbf{K}_{\\textbf{X} \\textbf{Y}}$\n",
        "\n",
        "**Covariance matrix as a linear operator**: \n",
        "* Assumptions:\n",
        "    * $\\textbf{X}$ is a random vector with covariance $\\text{cov}(\\textbf{X})$\n",
        "    * $\\textbf{c}$ and $\\textbf{d}$ are two vectors\n",
        "* Conclusion:\n",
        "    * $\\textbf{c}^T \\text{cov}(\\textbf{X}) = \\text{cov}(\\textbf{c}^T \\textbf{X}, \\textbf{X})$\n",
        "    * $\\text{cov}(\\textbf{X}) \\textbf{c} = \\text{cov}(\\textbf{X}, \\textbf{c}^T \\textbf{X})$\n",
        "* Consequence: $\\textbf{d}^T \\text{cov}(\\textbf{X}) \\textbf{c} = \\text{cov}(\\textbf{d}^T \\textbf{X}, \\textbf{c}^T \\textbf{X})$\n",
        "\n",
        "**Properties**:\n",
        "* Assumptions:\n",
        "    * $X = (X_1, ..., X_n)$ is a random vector from distribution $P$\n",
        "    * $Y = (Y_1, ..., Y_n)$ is a random vector from distribution $P$\n",
        "    * $\\textbf{S}$ is the covariance matrix of $P$\n",
        "    * $\\mu$ is the mean of $P$\n",
        "* Properties:\n",
        "    * $\\text{Var}(\\sum_i X_i) = \\sum_{i, j} \\text{Cov}(X_i, X_j)$\n",
        "    * $x^T \\textbf{S} x$ gives the variance of $x$, in the sense that $x^T \\textbf{S} x = \\text{Var}(\\sum_i x_i X_i)$\n",
        "        * Explain:\n",
        "            * $x = \\sum_i x_i e_i$ where $e_i$ is the $i$-th column of the identity matrix\n",
        "            * From above, $x^T \\textbf{S} x = \\sum_{i, j} x_i x_j (e_i^T \\textbf{S} e_j)$ \n",
        "            \n",
        "            $\\hspace{3.3cm} = \\sum_{i, j} x_i x_j \\text{Cov}(X_i, X_j)$ \n",
        "            \n",
        "            $\\hspace{3.3cm} = \\sum_{i, j} \\text{Cov}(x_i X_i, x_j X_j)$\n",
        "            \n",
        "            $\\hspace{3.3cm} = \\text{Var}(\\sum_i x_i X_i)$\n",
        "            \n",
        "  * $\\textbf{S}$ characterizes a hyperellipsoid which can be used to approximate data distribution shape\n",
        "    * Explain:\n",
        "        * Consider the SVD of $\\textbf{S} = \\textbf{P} \\textbf{D} \\textbf{P}^T$\n",
        "            * Due to symmetry, $\\textbf{P} \\textbf{D} \\textbf{P}^T$ is also the diagonalization of $\\textbf{S}$\n",
        "        * Consider $\\sigma_i$ and $v_i$ as eigenvalues and eigenvectors of $\\textbf{S}$\n",
        "        * $x^T \\textbf{S} x$ gives the variance of $x$ (as stated above)\n",
        "\n",
        "        $\\hspace{1.0cm} \\rightarrow v_i^T \\textbf{S} v_i = \\sigma_i \\|v_i\\|_2^2$\n",
        "        * From above, the distribution of the data can be approximated as a hyperellipsoid $E: \\sum_i \\frac{x_i}{\\sigma_i} = 1$ w.r.t the basis formed by the columns of $\\textbf{P}$\n",
        "\n",
        "# Discussion\n",
        "**From covariance matrix to PCA**:\n",
        "* Idea: reduce the dimension of data so that we can keep as much variance as possible\n",
        "* Method:\n",
        "    * Assumptions:\n",
        "        * $\\textbf{X} = \\begin{bmatrix} \\textbf{x}_1 && \\cdots && \\textbf{x}_n \\end{bmatrix}$ is the matrix of data\n",
        "        * $\\bar{\\textbf{x}}_n = \\frac{1}{n} \\sum_i \\textbf{x}_i$ is the sample mean\n",
        "    * Procedure:\n",
        "        * Step 1: normalize the data by the formula\n",
        "        \n",
        "        $\\hspace{3.0cm} \\hat{\\textbf{x}}_i = \\textbf{x}_i - \\bar{\\textbf{x}}_n$\n",
        "        * Step 2: compute the sample covariance matrix of the normalized data\n",
        "        \n",
        "        $\\hspace{3.0cm} \\textbf{S} = \\frac{1}{n} \\sum_i \\hat{\\textbf{x}}_i \\hat{\\textbf{x}}_i^T$ (or in vectorized form $\\textbf{S} = \\frac{1}{n} \\hat{\\textbf{X}} \\hat{\\textbf{X}}^T$)\n",
        "        * Step 3: compute the SVD $\\textbf{S} = \\textbf{P} \\textbf{D} \\textbf{P}^T$ where diagonal entries of $\\textbf{D}$ are sorted in decreasing order\n",
        "        * Step 4: take the first $k$ columns of $\\textbf{P}$ (i.e. corresponding to the $k$ largest eigenvalues of $\\textbf{S}$) to form a basis\n",
        "        * Step 5: for any new example $\\textbf{x}$, write $\\textbf{x}$ as $\\sum_{i = 1}^k c_i \\textbf{P}_{:,i}$ then take $\\textbf{c} = (c_1, ..., c_k)$ as the reduced version of $\\textbf{x}$\n",
        "* Intuition: project the data distribution, which is approximated by an hyperellipsoid, onto another basis which preserves as much variance as possible\n",
        "\n",
        "**From covariance matrix to Mahalanobis distance**:\n",
        "* Derivation:\n",
        "    * Consider the SVD $\\textbf{S} = \\textbf{P} \\textbf{D} \\textbf{D}^T$\n",
        "    * Consider $x$ w.r.t the basis formed by the columns of $\\textbf{P}$\n",
        "        * Formal: $x = \\sum_i c_i \\textbf{P}_{:,i} = \\textbf{P} \\textbf{c}$\n",
        "    * $x^T \\textbf{S}^{-1} x = \\textbf{c}^T \\textbf{D}^{-1} \\textbf{c} = \\|\\textbf{D}^{-1/2} \\textbf{c}\\|_2^2$\n",
        "\n",
        "    $\\hspace{1.0cm} \\rightarrow x^T \\textbf{S}^{-1} x$ is another measure of relative distance from the distribution characterized by $\\textbf{S}$ and $x$\n",
        "* Intuition:\n",
        "    * Step 1: we plot $x$ w.r.t to the basis formed by the columns of $\\textbf{P}$ (i.e. its coordinate would be $\\textbf{c}$)\n",
        "    * Step 2: scale down $c_i$ by $\\sigma_i$ (i.e. the variance of the data in the direction of $\\textbf{P}_{:, i}$)\n",
        "    * Step 3: compute $\\|\\textbf{c}\\|$ after scaling\n",
        "\n",
        "**Singularity of covariance matrix**: \n",
        "* Assumptions:\n",
        "    * $\\hat{\\textbf{x}}_i = \\textbf{x}_i - \\bar{\\textbf{x}}_n$\n",
        "    * $\\textbf{S} = \\frac{1}{n} \\sum_i \\hat{\\textbf{x}}_i \\hat{\\textbf{x}}_i^T$ \n",
        "        * Vectorized form: $\\textbf{S} = \\frac{1}{n} \\hat{\\textbf{X}} \\hat{\\textbf{X}}^T$\n",
        "* Conclusion: if the sample size $m$ is smaller than $n$\n",
        "\n",
        "$\\hspace{1.0cm} \\rightarrow$ The sample covariance $\\textbf{S}$ is singular\n",
        "* Explain: see the Matrix rank note"
      ]
    }
  ]
}